{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip install \"../input/keras-application/Keras_Applications-1.0.8-py3-none-any.whl\"\n!pip install \"../input/efficientnet111/efficientnet-1.1.1-py3-none-any.whl\"\n#!pip install \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n#!pip install \"../input/hpapytorchzoozip/pytorch_zoo-master\"\n#!pip install \"../input/tfexplainforoffline/tf_explain-0.2.1-py3-none-any.whl\"","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-06T15:54:38.234783Z","iopub.execute_input":"2021-09-06T15:54:38.235105Z","iopub.status.idle":"2021-09-06T15:54:50.545332Z","shell.execute_reply.started":"2021-09-06T15:54:38.235075Z","shell.execute_reply":"2021-09-06T15:54:50.544431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T16:11:28.511726Z","iopub.execute_input":"2021-09-06T16:11:28.512055Z","iopub.status.idle":"2021-09-06T16:11:28.762429Z","shell.execute_reply.started":"2021-09-06T16:11:28.512027Z","shell.execute_reply":"2021-09-06T16:11:28.761374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom PIL import Image\nfrom pathlib import Path\n%matplotlib inline\n\nimport random\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm.auto import tqdm\n\n# Modules for Deep Learning & Computer Vision\n\nimport tensorflow as tf\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom efficientnet.keras import EfficientNetB0\nfrom keras.layers import Dense, Flatten\nfrom keras.models import Model, load_model\n\nimport cv2\nfrom albumentations import Compose, VerticalFlip, HorizontalFlip, Rotate, GridDistortion\nfrom IPython.display import Image, display\nfrom tensorflow.python.framework import ops\n\n\n# Modules to encode predictions\n#from pycocotools import _mask as coco_mask\n#import base64\n#import zlib\n#import typing as t\n\n\nprint(\"Done--------\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-06T15:54:50.846351Z","iopub.execute_input":"2021-09-06T15:54:50.846745Z","iopub.status.idle":"2021-09-06T15:54:50.860748Z","shell.execute_reply.started":"2021-09-06T15:54:50.846705Z","shell.execute_reply":"2021-09-06T15:54:50.859655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to use GPU, check in top right of Kaggle notebook with 3 points, select 'accelerator' and change cpu by \"cuda\"\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.862551Z","iopub.execute_input":"2021-09-06T15:54:50.86335Z","iopub.status.idle":"2021-09-06T15:54:50.870246Z","shell.execute_reply.started":"2021-09-06T15:54:50.863271Z","shell.execute_reply":"2021-09-06T15:54:50.869312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Some hands-on data vizualisation with train set","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Vizualising some images","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"../input/hpa-single-cell-image-classification/train.csv\")\ndata_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.874416Z","iopub.execute_input":"2021-09-06T15:54:50.875067Z","iopub.status.idle":"2021-09-06T15:54:50.918058Z","shell.execute_reply.started":"2021-09-06T15:54:50.875025Z","shell.execute_reply":"2021-09-06T15:54:50.917171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Each image is uniquely identified by its \"ID\" with corresponding labels (most of the time multi-labels)**","metadata":{}},{"cell_type":"code","source":"data_train.shape # contains ID of 21806 images which are superposition of 4 chanels (filterred images) RGBY.","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.921261Z","iopub.execute_input":"2021-09-06T15:54:50.921609Z","iopub.status.idle":"2021-09-06T15:54:50.927055Z","shell.execute_reply.started":"2021-09-06T15:54:50.921574Z","shell.execute_reply":"2021-09-06T15:54:50.925769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\n'0. Nucleoplasm',\n'1. Nuclear membrane',\n'2. Nucleoli',\n'3. Nucleoli fibrillar center',\n'4. Nuclear speckles',\n'5. Nuclear bodies',\n'6. Endoplasmic reticulum',\n'7. Golgi apparatus',\n'8. Intermediate filaments',\n'9. Actin filaments',\n'10. Microtubules',\n'11. Mitotic spindle',\n'12. Centrosome',\n'13. Plasma membrane',\n'14. Mitochondria',\n'15. Aggresome',\n'16. Cytosol',\n'17. Vesicles and punctate cytosolic patterns',\n'18. Negative'\n] ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.928825Z","iopub.execute_input":"2021-09-06T15:54:50.929194Z","iopub.status.idle":"2021-09-06T15:54:50.938156Z","shell.execute_reply.started":"2021-09-06T15:54:50.929148Z","shell.execute_reply":"2021-09-06T15:54:50.936304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(n):\n    return labels[int(n)]\n\nlist(map(translate,[8,5,0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.939782Z","iopub.execute_input":"2021-09-06T15:54:50.940492Z","iopub.status.idle":"2021-09-06T15:54:50.95302Z","shell.execute_reply.started":"2021-09-06T15:54:50.94045Z","shell.execute_reply":"2021-09-06T15:54:50.952103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train[\"Labels_names\"] = data_train[\"Label\"].apply(lambda x : x.split(\"|\")).apply(lambda x : list(map(translate,x)))\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:50.954584Z","iopub.execute_input":"2021-09-06T15:54:50.95501Z","iopub.status.idle":"2021-09-06T15:54:51.01629Z","shell.execute_reply.started":"2021-09-06T15:54:50.954972Z","shell.execute_reply":"2021-09-06T15:54:51.015432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing 3 channels corresponding to one image + target (green title)\n# n correspond to the number of the image = index in data_train\n\n\ndef image_channels(n):\n    fig, ax = plt.subplots(1, 4, figsize=(30, 20), subplot_kw=dict(xticks=[], yticks=[]))\n    \n    img_blue = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_blue.png\".format(data_train.loc[n,\"ID\"])))\n    img_green = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_green.png\".format(data_train.loc[n,\"ID\"])))\n    img_red = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_red.png\".format(data_train.loc[n,\"ID\"])))\n    img_yellow = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_yellow.png\".format(data_train.loc[n,\"ID\"])))\n    \n    ax[0].imshow(img_blue)\n    ax[0].set_title('Blue filter = Nucleus', size=20)\n    ax[1].imshow(img_green)\n    ax[1].set_title(\"{}\".format(data_train.loc[n,\"Labels_names\"]), size=16 , color = \"green\")\n    ax[2].imshow(img_red)\n    ax[2].set_title('Red filter = Microtubules', size=20)\n    ax[3].imshow(img_yellow)\n    ax[3].set_title('Yellow filter = Endoplasmic Reticulum', size=20)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:51.018065Z","iopub.execute_input":"2021-09-06T15:54:51.018423Z","iopub.status.idle":"2021-09-06T15:54:51.027392Z","shell.execute_reply.started":"2021-09-06T15:54:51.018387Z","shell.execute_reply":"2021-09-06T15:54:51.026325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_channels(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:51.029077Z","iopub.execute_input":"2021-09-06T15:54:51.02979Z","iopub.status.idle":"2021-09-06T15:54:52.835359Z","shell.execute_reply.started":"2021-09-06T15:54:51.029746Z","shell.execute_reply":"2021-09-06T15:54:52.834519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Comparison of target image with merged RGB channels","metadata":{}},{"cell_type":"code","source":"def compare(n):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 25),subplot_kw=dict(xticks=[], yticks=[]))\n    \n    img_blue = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_blue.png\".format(data_train.loc[n,\"ID\"])))\n    img_green = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_green.png\".format(data_train.loc[n,\"ID\"])))\n    img_red = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_red.png\".format(data_train.loc[n,\"ID\"])))\n    img_yellow = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_yellow.png\".format(data_train.loc[n,\"ID\"])))\n    \n    #ax[1].imshow(img_green)\n    #ax[1].set_title(\"{}\".format(data_train.loc[n,\"Labels_names\"]), size=16 , color = \"green\")\n    \n    # print a complete image according to Green channel (protein labels)\n    img0 = plt.imread(Path(\"../input/hpa-single-cell-image-classification/train/{}_green.png\".format(data_train.loc[n,\"ID\"])))\n    ax[0].imshow(img0)\n    ax[0].set_title(\"Image G / Target: {}\".format(data_train.loc[n,\"Labels_names\"]), size=16 , color = \"green\")\n    \n    # print a complete image according to 3 channels Red Green Blue\n    img1 = np.dstack((img_red, img_green, img_blue))\n    ax[1].imshow(img1)\n    ax[1].set_title(\"Image RGB / Target: {}\".format(data_train.loc[n,\"Labels_names\"]), size=16 , color = \"green\")\n       \n# I think important to recall that blue + yellow = green, thus when we put together channels R/Y/B, protein of interest appears ?","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:52.83698Z","iopub.execute_input":"2021-09-06T15:54:52.837312Z","iopub.status.idle":"2021-09-06T15:54:52.848535Z","shell.execute_reply.started":"2021-09-06T15:54:52.837279Z","shell.execute_reply":"2021-09-06T15:54:52.84743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:52.850075Z","iopub.execute_input":"2021-09-06T15:54:52.850448Z","iopub.status.idle":"2021-09-06T15:54:54.324858Z","shell.execute_reply.started":"2021-09-06T15:54:52.850416Z","shell.execute_reply":"2021-09-06T15:54:54.323719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Define methods to open 4-channel images from train/test set","metadata":{}},{"cell_type":"code","source":"TEST_IMGS_FOLDER = '../input/hpa-single-cell-image-classification/test/'\nTRAIN_IMGS_FOLDER = '../input/hpa-single-cell-image-classification/train/'\nIMG_HEIGHT = IMG_WIDTH = 128 # other good parameter is 512\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:54.326521Z","iopub.execute_input":"2021-09-06T15:54:54.32689Z","iopub.status.idle":"2021-09-06T15:54:54.332025Z","shell.execute_reply.started":"2021-09-06T15:54:54.326854Z","shell.execute_reply":"2021-09-06T15:54:54.331254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function that reads RGBY image (4-channels) with resizing\ndef open_rgby(image_id): \n    colors = ['red','green','blue','yellow']\n    img = [cv2.imread(os.path.join(TRAIN_IMGS_FOLDER , f'{image_id}_{color}.png'), cv2.IMREAD_GRAYSCALE) for color in colors]\n    img = np.stack(img, axis=-1)\n    img_resized = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n    img_resized=img_resized/255\n    return img_resized\n\n# same function without resizing for test set\ndef open_rgby_test(image_id): \n    colors = ['red','green','blue','yellow']\n    img = [cv2.imread(os.path.join(TEST_IMGS_FOLDER, f'{image_id}_{color}.png'), cv2.IMREAD_GRAYSCALE) for color in colors]\n    img = np.stack(img, axis=-1)\n    return img\n\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:54.333784Z","iopub.execute_input":"2021-09-06T15:54:54.334195Z","iopub.status.idle":"2021-09-06T15:54:54.344311Z","shell.execute_reply.started":"2021-09-06T15:54:54.334149Z","shell.execute_reply":"2021-09-06T15:54:54.343389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With 4 channels RGBY (resized to 128x128)\nimg = open_rgby(\"5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0\")\nplt.imshow(img)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:54.346339Z","iopub.execute_input":"2021-09-06T15:54:54.346856Z","iopub.status.idle":"2021-09-06T15:54:54.712234Z","shell.execute_reply.started":"2021-09-06T15:54:54.346817Z","shell.execute_reply":"2021-09-06T15:54:54.711519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Image Segmentation\n\nNow we will create our first multi-label mask. The task is not easy at all, but fortunately, the Human Cell Group provide us a powerful module \"HPACellSegmentator\" already tested for a previous challenge. The module is available on GitHub at https://github.com/CellProfiling/HPA-Cell-Segmentation","metadata":{}},{"cell_type":"code","source":"!pip install https://github.com/CellProfiling/HPA-Cell-Segmentation/archive/master.zip","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:54:54.713894Z","iopub.execute_input":"2021-09-06T15:54:54.714254Z","iopub.status.idle":"2021-09-06T15:55:03.664691Z","shell.execute_reply.started":"2021-09-06T15:54:54.714216Z","shell.execute_reply":"2021-09-06T15:55:03.66382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"../input/hpacellsegmentatormaster/HPA-Cell-Segmentation-master\"","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:03.668425Z","iopub.execute_input":"2021-09-06T15:55:03.668704Z","iopub.status.idle":"2021-09-06T15:55:11.658091Z","shell.execute_reply.started":"2021-09-06T15:55:03.668666Z","shell.execute_reply":"2021-09-06T15:55:11.65701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import hpacellseg.cellsegmentator as cellsegmentator\nfrom hpacellseg.utils import label_cell, label_nuclei","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:11.661747Z","iopub.execute_input":"2021-09-06T15:55:11.66203Z","iopub.status.idle":"2021-09-06T15:55:11.668764Z","shell.execute_reply.started":"2021-09-06T15:55:11.662002Z","shell.execute_reply":"2021-09-06T15:55:11.667721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#load of the pre-trained weights required to compute the segmentations\nNUC_MODEL = '../input/hpacellsegmentatormodelweights/dpn_unet_nuclei_v1.pth'\nCELL_MODEL = '../input/hpacellsegmentatormodelweights/dpn_unet_cell_3ch_v1.pth'\n\n\n#instanciation of an object \"segmentator\"\n\nsegmentator = cellsegmentator.CellSegmentator(\n    NUC_MODEL,\n    CELL_MODEL,\n    scale_factor=0.25,\n    device=\"cpu\",    # to use GPU, check in top right of Kaggle notebook with 3 points, select 'accelerator' and change cpu by \"cuda\"\n    padding=False,\n    multi_channel_model=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:11.670256Z","iopub.execute_input":"2021-09-06T15:55:11.6706Z","iopub.status.idle":"2021-09-06T15:55:23.17111Z","shell.execute_reply.started":"2021-09-06T15:55:11.670564Z","shell.execute_reply":"2021-09-06T15:55:23.170313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = data_train.loc[0,\"ID\"] # just need to change the number of image (here row 1)\n\nred = f\"../input/hpa-single-cell-image-classification/train/{image_id}_red.png\" # the f'string trick avoid to use a {}.format()\ngreen = f\"../input/hpa-single-cell-image-classification/train/{image_id}_green.png\"\nblue = f\"../input/hpa-single-cell-image-classification/train/{image_id}_blue.png\"\n\nimages = [[red], [green], [blue]]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:23.17451Z","iopub.execute_input":"2021-09-06T15:55:23.174786Z","iopub.status.idle":"2021-09-06T15:55:23.182141Z","shell.execute_reply.started":"2021-09-06T15:55:23.17476Z","shell.execute_reply":"2021-09-06T15:55:23.181227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.loc[0,\"Label\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:23.183469Z","iopub.execute_input":"2021-09-06T15:55:23.183865Z","iopub.status.idle":"2021-09-06T15:55:23.19563Z","shell.execute_reply.started":"2021-09-06T15:55:23.183815Z","shell.execute_reply":"2021-09-06T15:55:23.194651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some useful methods of the HPA CellSegmentator module\n\n- **pred_nuclei :** The function takes a list of image arrays or a list of string paths to images. If the image arrays are 3 channels, the nuclei should be in the third (blue) channel.Returns a list of the neural network outputs of the nuclei segmentation. The images are on the format (3, H, W). The three channels are as follows [<Unused>, touching-nuclei, Nuclei-segmentation].\n\n- **pred_cells :** The function takes a list of three lists as input. The lists should contain either image arrays or string paths, in the order of microtubules, endoplasmic reticulum, and nuclei.\nReturns a list of the neural network outputs of the cell segmentations. The images are on the format (3, H, W). The three channels for the cell segmentation are as follows [<Unused>, touching-cells, Cell-segmentation].\n\nNote that both these functions assume that all input images are of the same shape!!\n    \n\n- **label_cell :** Input with the nuclei and cell prediction for an image. Returns the labeled nuclei and cell mask arrays as a tuple. As with label_nuclei, the background is 0s and other numbers indicates which cell is there. The same cell will have the same number in both arrays.","metadata":{}},{"cell_type":"code","source":"# prediction masks for nuclei mask, return a list of array\nnuc_segmentations = segmentator.pred_nuclei(images[2]) # apply segmentator model to the nuclei channel blue corresponding to 2\n\n# prediction masks for full cell, return a list of array\ncell_segmentations = segmentator.pred_cells(images) # apply segmentator model to the full object images corresponding to 3 channels RGB ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:23.197139Z","iopub.execute_input":"2021-09-06T15:55:23.197699Z","iopub.status.idle":"2021-09-06T15:55:38.837409Z","shell.execute_reply.started":"2021-09-06T15:55:23.197646Z","shell.execute_reply":"2021-09-06T15:55:38.836562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nuc_segmentations[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:38.838838Z","iopub.execute_input":"2021-09-06T15:55:38.839171Z","iopub.status.idle":"2021-09-06T15:55:38.846699Z","shell.execute_reply.started":"2021-09-06T15:55:38.839136Z","shell.execute_reply":"2021-09-06T15:55:38.845516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cell_segmentations[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:38.848496Z","iopub.execute_input":"2021-09-06T15:55:38.849271Z","iopub.status.idle":"2021-09-06T15:55:38.857555Z","shell.execute_reply.started":"2021-09-06T15:55:38.849158Z","shell.execute_reply":"2021-09-06T15:55:38.856338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# post-processing\nnuclei_mask, cell_mask = label_cell(nuc_segmentations[0], cell_segmentations[0]) # label_cell is a method of CellSegmentator","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:38.859162Z","iopub.execute_input":"2021-09-06T15:55:38.859635Z","iopub.status.idle":"2021-09-06T15:55:43.388462Z","shell.execute_reply.started":"2021-09-06T15:55:38.859599Z","shell.execute_reply":"2021-09-06T15:55:43.387726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the preceding single image(n=1) with 3 channels RGB + mask\n# make a function to include number of image\nn=0\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 25),subplot_kw=dict(xticks=[], yticks=[]))\n\nred_img = plt.imread(Path(red))    \ngreen_img = plt.imread(Path(green))    \nblue_img = plt.imread(Path(blue))\n\nax[0].imshow(cell_mask, alpha=0.6)\nax[0].set_title(\"Multi-cell Mask with labels: {}\".format(data_train.loc[n,\"Label\"]), size=16 , color = \"green\")\n\nimg1 = np.dstack((red_img,green_img , blue_img))\nax[1].imshow(img1)\nax[1].set_title(\"Image RGB with labels: {}\".format(data_train.loc[n,\"Label\"]), size=16 , color = \"green\")\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:43.389644Z","iopub.execute_input":"2021-09-06T15:55:43.389982Z","iopub.status.idle":"2021-09-06T15:55:44.505427Z","shell.execute_reply.started":"2021-09-06T15:55:43.389947Z","shell.execute_reply":"2021-09-06T15:55:44.504563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. More Exploratory Data Analysis (EDA)\n\n## 2.1 Encoding the labels\n\nRecall that our dataset mainly consists of\n1. A file train.csv : 21806 images ID\n2. A folder train : 21806x4 png files corresponding to channels RGBY, 1 ID = 1 image = 4 channels RGBY\n\nAccording to the description of the problem, labels may be regrouped by family : \n\n- Nucleus [0,1,2,3,4,5]  \n- Reticulum [6]  \n- Tubule [10,11] \n- Cytoplasm [8,9,12,14,15,16] \n- Secretory [7,13,17]\n\nCommentaire: Pour ces familles de labels unique on crée 5 modèles, ces modèles prennent en entrée tableau bleu vert pour predire le noyau, jaune vert pour predire le reticulum, rouge vert pour predire le tubule et rouge vert bleu jaune pour predire les 2 autres classes","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\ndef binarize_data(df):\n    mlb = MultiLabelBinarizer()\n    df['labels_list']=[list(map(int, i.split(\"|\"))) for i in df.Label]\n    mlb.fit(df['labels_list'].to_list())\n    Labels_binarized = mlb.transform(df['labels_list'])\n    return Labels_binarized\n\n\ndef label_choice(n):\n     data_train['Label_{}'.format(n)]=data_train['Multilabel'].apply(lambda x:x[n])","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.506694Z","iopub.execute_input":"2021-09-06T15:55:44.507144Z","iopub.status.idle":"2021-09-06T15:55:44.513974Z","shell.execute_reply.started":"2021-09-06T15:55:44.507108Z","shell.execute_reply":"2021-09-06T15:55:44.513129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding target into array with multibinarizer (e.g we binarize each label)\n\nmultilabel_list = binarize_data(data_train).tolist()\ndata_train['Multilabel']=[x for x in multilabel_list]\ndata_train['Multilabel']=data_train['Multilabel'].apply(lambda x:x[0:len(x)-1])\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.515243Z","iopub.execute_input":"2021-09-06T15:55:44.515785Z","iopub.status.idle":"2021-09-06T15:55:44.685129Z","shell.execute_reply.started":"2021-09-06T15:55:44.515745Z","shell.execute_reply":"2021-09-06T15:55:44.684205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Splitting our dataset between data_monolabel and data_multilabel\n\nHere we split our train set into mono-label images and multi-label images in order to simplify our problem.","metadata":{}},{"cell_type":"code","source":"# Looking distribution of each label in data_train : make graph\n\nnumber_img_per_target = []\npercentage = []\nfor i in range(18):\n    label_choice(i)\n    label='Label_{}'.format(i)\n    mask=(data_train[label]==1)\n    number_img_per_target.append(len(data_train.loc[mask,label]))\n    percentage.append(len(data_train.loc[mask,label])/21806)\n    percentage = [round(elem,3) for elem in percentage]\nprint(number_img_per_target)\nprint(percentage)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.686384Z","iopub.execute_input":"2021-09-06T15:55:44.686726Z","iopub.status.idle":"2021-09-06T15:55:44.908908Z","shell.execute_reply.started":"2021-09-06T15:55:44.686697Z","shell.execute_reply":"2021-09-06T15:55:44.907808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting data_train between Monolable and Multilabel\n\ndata_train[\"MonoLabel\"]=data_train['Multilabel'].apply(lambda x:1 if sum(x)==1 else 0)\n\ndata_monolabel=data_train[data_train[\"MonoLabel\"]==1]\ndata_multilabel=data_train[data_train[\"MonoLabel\"]==0]\n\n#data_multilabel=data_multilabel.sample(100) \n#data_train_croped=data_monolabel #.sample(10000, random_state=42)\n#data_train_croped.shape\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.910377Z","iopub.execute_input":"2021-09-06T15:55:44.91075Z","iopub.status.idle":"2021-09-06T15:55:44.946186Z","shell.execute_reply.started":"2021-09-06T15:55:44.910711Z","shell.execute_reply":"2021-09-06T15:55:44.94515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_monolabel.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.953595Z","iopub.execute_input":"2021-09-06T15:55:44.95388Z","iopub.status.idle":"2021-09-06T15:55:44.982112Z","shell.execute_reply.started":"2021-09-06T15:55:44.953852Z","shell.execute_reply":"2021-09-06T15:55:44.98124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10474 images with monolabel\ndata_monolabel.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.984863Z","iopub.execute_input":"2021-09-06T15:55:44.9855Z","iopub.status.idle":"2021-09-06T15:55:44.990993Z","shell.execute_reply.started":"2021-09-06T15:55:44.985458Z","shell.execute_reply":"2021-09-06T15:55:44.990074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' NUMBER OF IMAGES FOR EACH TARGET Label in data_monolabel'''\nliste_i=[]\nliste_val=[]\nfor i in range(18):\n    label='Label_{}'.format(i)\n    mask=(data_monolabel[label]==1)\n    x=data_monolabel.loc[mask,label].sum()\n    liste_i.append(i)\n    liste_val.append(x)\ndico={'x':liste_i,\"y\":liste_val}\ndf=pd.DataFrame(dico)\nsns.catplot(x='x',y='y', data=df, kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:44.992553Z","iopub.execute_input":"2021-09-06T15:55:44.993132Z","iopub.status.idle":"2021-09-06T15:55:45.320114Z","shell.execute_reply.started":"2021-09-06T15:55:44.99309Z","shell.execute_reply":"2021-09-06T15:55:45.319059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_multilabel.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.321536Z","iopub.execute_input":"2021-09-06T15:55:45.321904Z","iopub.status.idle":"2021-09-06T15:55:45.327848Z","shell.execute_reply.started":"2021-09-06T15:55:45.321867Z","shell.execute_reply":"2021-09-06T15:55:45.326777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_multilabel.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.329153Z","iopub.execute_input":"2021-09-06T15:55:45.32959Z","iopub.status.idle":"2021-09-06T15:55:45.359032Z","shell.execute_reply.started":"2021-09-06T15:55:45.329548Z","shell.execute_reply":"2021-09-06T15:55:45.357937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' NUMBER OF IMAGES FOR EACH TARGET Label in data_multilabel'''\n\nliste_i=[]\nliste_val=[]\nfor i in range(18):\n    label='Label_{}'.format(i)\n    mask=(data_multilabel[label]==1)\n    x=data_multilabel.loc[mask,label].sum()\n    liste_i.append(i)\n    liste_val.append(x)\ndico={'x':liste_i,\"y\":liste_val}\ndf=pd.DataFrame(dico)\nsns.catplot(x='x',y='y', data=df, kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.360553Z","iopub.execute_input":"2021-09-06T15:55:45.360911Z","iopub.status.idle":"2021-09-06T15:55:45.672169Z","shell.execute_reply.started":"2021-09-06T15:55:45.360876Z","shell.execute_reply":"2021-09-06T15:55:45.671406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this state of the analysis, we remark that the label distribution in mono-label images and multi-label images is highly unbalanced.\nSome labels, like label 0 which corresponds to nucleoplasm of the cell is very frequent. On the contrary, some labels are very rare, or even absent like label 11 in \nmono-label images. So this label will be very difficult to detect, as he is only visible in a mixture of other labels. For the training, it will be harder to identify it.\n\nNow we have our images (dataset), we have transformed all the labels (our target) with binary encoding, we have to give this input to some dense neural network in order \nto make some predictions (first at the image-level, not cell level).\n\n### For the next part of this review, we will only study **mono-label images**\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Preprocessing our mono-label images dataset\n\nMain steps\n\n- Equilibrate our dataset **data_monolabel** with a best possible balance of labels\n- Split into train/validation set (a test set is given apart)\n\n\n","metadata":{}},{"cell_type":"code","source":"''' SELECT SUB DATASET WITH ONLY label n'''\ndef data_monolabel_sub(n):\n    label='Label_{}'.format(n)\n    mask=(data_monolabel[label]==1)\n    data_sub = data_monolabel.loc[mask]\n    return(data_sub)\n\n''' SELECT SUB DATASET of EQUAL SIZE without label n'''\ndef data_monolabel_sub_nolabel(n):\n    label='Label_{}'.format(n)\n    mask=(data_monolabel[label]==0)\n    data_sub_nolabel = data_monolabel.loc[mask]\n    data_sub_nolabel = data_monolabel.sample(len(data_monolabel_sub(n)), random_state=42)\n    return(data_sub_nolabel)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.673731Z","iopub.execute_input":"2021-09-06T15:55:45.674081Z","iopub.status.idle":"2021-09-06T15:55:45.680231Z","shell.execute_reply.started":"2021-09-06T15:55:45.674044Z","shell.execute_reply":"2021-09-06T15:55:45.67921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our global dataset will be the disjoint union (merging) of:\n1. data_monolabel_sub(n) = dataset with label n PRESENT in the image ID\n2. data_monolabel_sub_nolabel(n) = dataset with label n ABSENT of the image ID\n\nWe took same proportions (50/50 percent) in order to train our model. The model will learn equally to detect:\n\n- images with the protein label (n) \n- or images without the protein label (n)","metadata":{}},{"cell_type":"code","source":"''' EXECUTE, choose your label number n before'''\n\nn=7 # here this is label 7\ndata_train_croped = pd.concat([data_monolabel_sub(7),data_monolabel_sub_nolabel(7)], ignore_index=True)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.681895Z","iopub.execute_input":"2021-09-06T15:55:45.682348Z","iopub.status.idle":"2021-09-06T15:55:45.703585Z","shell.execute_reply.started":"2021-09-06T15:55:45.682309Z","shell.execute_reply":"2021-09-06T15:55:45.702713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train_croped.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.704892Z","iopub.execute_input":"2021-09-06T15:55:45.705239Z","iopub.status.idle":"2021-09-06T15:55:45.71073Z","shell.execute_reply.started":"2021-09-06T15:55:45.705202Z","shell.execute_reply":"2021-09-06T15:55:45.709774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train_croped=data_train_croped.sample(1000, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.712314Z","iopub.execute_input":"2021-09-06T15:55:45.712825Z","iopub.status.idle":"2021-09-06T15:55:45.720014Z","shell.execute_reply.started":"2021-09-06T15:55:45.71277Z","shell.execute_reply":"2021-09-06T15:55:45.719006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef dataset_split(n): \n    y=data_train_croped['Label_{}'.format(n)]\n    X=data_train_croped[\"ID\"]\n    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42,stratify=y)\n    return (X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.721879Z","iopub.execute_input":"2021-09-06T15:55:45.722461Z","iopub.status.idle":"2021-09-06T15:55:45.729435Z","shell.execute_reply.started":"2021-09-06T15:55:45.72242Z","shell.execute_reply":"2021-09-06T15:55:45.728434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' EXECUTE'''\n\n#X_train_0, X_test_0, y_train_0, y_test_0 = dataset_split(0)\nX_train_7, X_test_7, y_train_7, y_test_7 = dataset_split(7)\n#X_train_9, X_test_9, y_train_9, y_test_9 = dataset_split(9)\n#X_train_12, X_test_12, y_train_12, y_test_12 = dataset_split(12)\n#X_train_14, X_test_14, y_train_14, y_test_14 = dataset_split(14)\n#X_train_15, X_test_15, y_train_15, y_test_15 = dataset_split(15)\n#X_train_16, X_test_16, y_train_16, y_test_16 = dataset_split(16)\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.73096Z","iopub.execute_input":"2021-09-06T15:55:45.731385Z","iopub.status.idle":"2021-09-06T15:55:45.743835Z","shell.execute_reply.started":"2021-09-06T15:55:45.731346Z","shell.execute_reply":"2021-09-06T15:55:45.742508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Preparation of Dataset with Tensorflow\n\nNow we have established our train and test sets, we need to transform the images into suitable format for input into our model (neural network to determine later)\n\nMain steps:\n\n- Transform our images into tensor with Tensorflow\n- Decide which neural network we want to use (build from scratch? how deep? Architecture of layers? Using pre-trained model ?...)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:43:39.367801Z","iopub.execute_input":"2021-09-04T16:43:39.368142Z","iopub.status.idle":"2021-09-04T16:43:39.371815Z","shell.execute_reply.started":"2021-09-04T16:43:39.368113Z","shell.execute_reply":"2021-09-04T16:43:39.370791Z"}}},{"cell_type":"code","source":"# Because our computations are going to be tedious, we need to clean some variables not useful anymore\ndel data_train\n#del specified_class_names\n#del class_names\ndel multilabel_list\ndel binarize_data\ndel label_choice\ndel data_monolabel\ndel data_multilabel\ndel data_train_croped\n\ngc.collect()\n\nprint(\"Done--------\")\n# Remark :  we can't delete call functions like data_monolabel_sub(n)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:45.745757Z","iopub.execute_input":"2021-09-06T15:55:45.746222Z","iopub.status.idle":"2021-09-06T15:55:46.049956Z","shell.execute_reply.started":"2021-09-06T15:55:45.746178Z","shell.execute_reply":"2021-09-06T15:55:46.049088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Transforming our dataset into Tensorflow.dataset","metadata":{}},{"cell_type":"code","source":"'''\n##############################################################\n# EXECUTE Tensor slices for TRAIN set / label (n)\n##############################################################\n'''\n\nds_train_7 = tf.data.Dataset.from_tensor_slices([(open_rgby(str(x))) for x in tqdm(X_train_7)])","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:55:46.053144Z","iopub.execute_input":"2021-09-06T15:55:46.053549Z","iopub.status.idle":"2021-09-06T15:57:37.521069Z","shell.execute_reply.started":"2021-09-06T15:55:46.053506Z","shell.execute_reply":"2021-09-06T15:57:37.519627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n##############################################################\n# EXECUTE Tensor slices for VALIDATION set / label (n)\n##############################################################\n'''\nds_val_7 = tf.data.Dataset.from_tensor_slices([(open_rgby(str(x))) for x in tqdm(X_test_7)])","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.522309Z","iopub.status.idle":"2021-09-06T15:57:37.522795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n##############################################################\n# EXECUTE Tensor slices for TARGET of TRAIN set / label (n)\n##############################################################\n'''\nds_label_7 = tf.data.Dataset.from_tensor_slices(y_train_7)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.524041Z","iopub.status.idle":"2021-09-06T15:57:37.52474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n##############################################################\n# EXECUTE Tensor slices for TARGET of VALIDATION set / label (n)\n##############################################################\n'''\nds_label_val_7 = tf.data.Dataset.from_tensor_slices(y_test_7)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.526444Z","iopub.status.idle":"2021-09-06T15:57:37.527088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zip train (categorical) and label_train (target)\n\nds_full_7 = tf.data.Dataset.zip((ds_train_7,ds_label_7))\n\npath_save='./train_ds_full_7'\ntf.data.experimental.save(ds_full_7, path_save, compression= 'gzip', shard_func=None)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.528356Z","iopub.status.idle":"2021-09-06T15:57:37.529035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zip validation (categorical) and label_validation (target)'''\n\nds_full_val_7 = tf.data.Dataset.zip((ds_val_7,ds_label_val_7))\n\npath_save='./train_ds_full_val_7'\ntf.data.experimental.save(ds_full_val_7, path_save, compression= 'gzip', shard_func=None)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.530443Z","iopub.status.idle":"2021-09-06T15:57:37.531228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' TO RELOAD DATA'''\n\n#ds_full_val_7 = tf.data.experimental.load(path_save,tf.TensorSpec(shape=[128,128,4], dtype=tf.uint8), compression='gzip')\n#ds_full_val_7","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.532497Z","iopub.status.idle":"2021-09-06T15:57:37.533339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Data augmentation\n\nFor RARE labels, that is very few number compared to the number of images, we need to add some data, in order to have \nenough images for training (at least 10 000).\nIndeed, some rare labels are present in only a few hundreds or one thousand images, which is too small for training. \nThe technique we use here is Data Augmentation, that is, we copy the number of images until the desired number we want for training :\n1000 images, copy 10 times --> 10 000 images\neach images copy is slightly modified (by some random resizing, brightness, rotate,...) in order to have **different** images for the entire dataset (10 000)\nThen, we will train our model for each label on a train set of 10 000 different images containing exactly one label.","metadata":{}},{"cell_type":"code","source":"# create fonction which randomly modify each copy\ndef data_aug_simple(image):\n    # rotations and flipping\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_crop(image, [128, 128, 4])\n    image=tf.image.rot90(image, k=1, name=None)\n    #image = tf.image.random_brightness(image, 0.05)\n    #image = tf.image.random_contrast(image, 0.5, 1.5)\n    return image\n\ndef data_aug(image, label):\n    # rotations and flipping\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_crop(image, [128, 128, 4])\n    image=tf.image.rot90(image, k=1, name=None)\n    #image = tf.image.random_brightness(image, 0.05)\n    #image = tf.image.random_contrast(image, 0.5, 1.5)\n    return image, label\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.534858Z","iopub.status.idle":"2021-09-06T15:57:37.535672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(10, 10), subplot_kw=dict(xticks=[], yticks=[])) \n   \nid=\"000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0\"\nimg=open_rgby(id)\nax[0].imshow(img)\nax[0].set_title('Image (normal)', size=12)\n    \nimg_aug = data_aug_simple(img)\nax[1].imshow(img_aug)\nax[1].set_title('Image (modified)', size=12)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.536999Z","iopub.status.idle":"2021-09-06T15:57:37.537713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#REMIND number of images in train set for your label choice (n)\nlen(ds_full_7)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.538991Z","iopub.status.idle":"2021-09-06T15:57:37.539811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attention, le nombre de répététion dépend du nombre d'élément dans le data set\n#N=7000\n#taille=data_train_croped.shape[0]*0.7\n#nb_repeat=int(N/len(ds_full))\n#nb_repeat\n\n\nN= 5000 # number of images in input\nnb_copy = int(N / len(ds_full_7))\nnb_copy","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.541074Z","iopub.status.idle":"2021-09-06T15:57:37.541901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here adapt the number of copy to the size of your dataset data_train_croped\n\n''' EXECUTE TO CREATE FULL DATASET for single label (n)'''\nds_full_7_augmented = ds_full_7.repeat(nb_copy)\nds_full_7_augmented = ds_full_7_augmented.map(data_aug, num_parallel_calls=4) # compute 4 operations in parallel, advise de set this number = CPU number (4)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.543175Z","iopub.status.idle":"2021-09-06T15:57:37.543993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check number of images of our full dataset for label n\nlen(ds_full_7_augmented)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.545254Z","iopub.status.idle":"2021-09-06T15:57:37.546098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Last step with Tensorflow : Shuffle and Batch Size","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\n\nds_full_ready_7 = ds_full_7_augmented.shuffle(len(ds_full_7_augmented)).batch(BATCH_SIZE)\nds_full_val_ready_7=ds_full_val_7.batch(BATCH_SIZE) # no need to shuffle for validation set, will not be trained\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.547496Z","iopub.status.idle":"2021-09-06T15:57:37.548223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking right format for input of our Neural Network\nds_full_ready_7","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.549562Z","iopub.status.idle":"2021-09-06T15:57:37.550293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_full_ready_7.take(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.551574Z","iopub.status.idle":"2021-09-06T15:57:37.552424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Definition and training of the model\n\nNow our dataset is suitably prepared as an input to some neural network, we need to find architecture of our model. \nFirst, it is well know (by experience) that image classification problems require deep neural networks in order to give acceptable results.\nIt means many layers of different kind and size, the choice of architecture is very complicated. The only thing we now, is that our **last** layer must be \na dense layer with 18 neurons, as our classification problem has 18 labels (we put label 18 - no protein- aside for the moment).\n\nOver the last few years, there have been a series of breakthroughs in the field of Computer Vision.Especially with the introduction of deep Convolutional neural networks, we are getting state of the art results on problems such as image classification and image recognition. So, over the years, researchers tend to make deeper neural networks(adding more layers) to solve such complex tasks and to also improve the classification/recognition accuracy. But, it has been seen that as we go adding on more layers to the neural network, it becomes difficult to train them and the accuracy starts saturating and then degrades also. Here, the EfficientNet model comes into rescue and helps solve this problem. \n\nIt was first introduced by leading researchers in Google AI in a recent paper (2019) https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for\nOther reference is https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n\n\nAn other excellent choice would have been ResNet, short for Residual Network which is a specific type of neural network that was introduced in 2015 by Microsoft AI researchers Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun in their paper “Deep Residual Learning for Image Recognition”, in order to solve the problem of the vanishing/exploding gradient. The link to ArXiv is here : https://arxiv.org/abs/1512.03385 \n\nWe will not discuss here the foundations and architecture of this model, which is far beyond our scope, but rather take it as a *state-of-the-art* Neural Network model for computer vision.\n\nDue to the enormous number of layers of this NN, we use pre-trained weights for all layers except the last dense layer that we will train to predict our labels.\n\n\n\nI'd like to share how to leverage pre-trained 3-channel Keras models to initialize a 4-channel model.\n\nIn the discussion forums the competition hosts have stressed the potential importance of all 4 colors, e.g. \"All images have all the four channels, and signals from the markers (blue, yellow, red) are present in all cells in the image, independent of the green channel that you are classifying, in order to help you identify where the cells are, as well as where certain structures and regions within the cells are. This can, in turn, help you to segment the cells and to classify each cell to one or more label(s) according to the signal in the green channel.\" link to the post https://www.kaggle.com/c/hpa-single-cell-image-classification/discussion/215736#1184158\n\n\n\nConsidering the size of training data, learning a deep 4-channel model with weights initialized at random might be problematic. But **all ImageNet-pre-trained models have 3-channels**. Then, we will initialize a 4-channel EfficientNet with weights reused from a pre-trained 3-channel model.\n\n\n\nMain steps :\n- load pre-trained weights for the RGB model\n- adapt to the RGBY model\n- freeze weights in order to train only the last dense layer\n- define learning rate schedule, optimizer\n- compile the model\n- train the model\n- display accuracy","metadata":{}},{"cell_type":"code","source":"''' EXECUTE'''\n\n# Loading model with weights pre-trained on ImageNet for RGB model\n# for Kaggle only : internet must be enabled\n\n\nDOWNLOAD_PRETRAINED_WEIGHTS = False\n\nweights_init = 'imagenet' if DOWNLOAD_PRETRAINED_WEIGHTS else None\n\nimagenet_model = EfficientNetB0(weights=weights_init, include_top=False, pooling='avg',\n                               input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\nrgb_model_output = Dense(1, activation='sigmoid')(imagenet_model.output)\nmodel_rgb = Model(inputs=imagenet_model.input, outputs=rgb_model_output)\n\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.5538Z","iopub.status.idle":"2021-09-06T15:57:37.554486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' EXECUTE'''\n\n#Model RGBY for label n : just indicate number n after model name: model_rgby_label_n \n\nfour_channel_effnet = EfficientNetB0(weights=None, include_top=False, pooling='avg', \n                                     input_shape=(IMG_HEIGHT, IMG_WIDTH, 4)) # base model\nmodel_rgby_output = Dense( 1, activation='sigmoid')(four_channel_effnet.output)\nmodel_rgby_label_7 = Model(inputs=four_channel_effnet.input, outputs=model_rgby_output)\nprint(\"Done--------\")\n#model_rgby_label_7.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.555794Z","iopub.status.idle":"2021-09-06T15:57:37.556594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' EXECUTE'''\n\n# Loading weights from imagenet model\nlink_imagenet = '../input/efficientnet-keras-dataset/weights/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5'\nimagenet_model = EfficientNetB0(weights= link_imagenet, include_top=False, pooling='avg', \n                                     input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n#imagenet_model.summary()\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.557953Z","iopub.status.idle":"2021-09-06T15:57:37.558643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' EXECUTE'''\n# Model RGB from which we load imagenet weigths (3 channels only)\nrgb_model_output = Dense(1, activation='sigmoid')(imagenet_model.output)\nmodel_rgb = Model(inputs=imagenet_model.input, outputs=rgb_model_output)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.559986Z","iopub.status.idle":"2021-09-06T15:57:37.560676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' EXECUTE'''\n# Extending weights from 3 channels to 4 channels\n# don't forget to precise the label in the model name: model_rgby_label_n\n\nfor layer in tqdm(model_rgby_label_7.layers, desc='Copying the pre-trained net weights..'):\n    if 'input' in layer.name or 'dense' in layer.name:\n        continue\n    elif layer.name == 'stem_conv':\n#         with graph_green.as_default():\n        kernels = model_rgb.get_layer('stem_conv').get_weights()[0]\n        kernels_extra_channel = np.concatenate((kernels, kernels[:,:,:1,:]), axis=-2)\n        layer.set_weights([kernels_extra_channel])\n    else:\n#         with graph_green.as_default():\n        weights_green = model_rgb.get_layer(layer.name).get_weights()\n        layer.set_weights(weights_green)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.562087Z","iopub.status.idle":"2021-09-06T15:57:37.56279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n#In order to freeze weights from pre-trained model, and train only last Dense layer\n\nfour_channel_effnet.trainable = False\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.564065Z","iopub.status.idle":"2021-09-06T15:57:37.564789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n# Let's create a learning rate schedule to decrease the learning rate as we train the model. \ninitial_learning_rate = 0.001\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=1000,\n    decay_rate=0.96,\n    staircase=True\n)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.566189Z","iopub.status.idle":"2021-09-06T15:57:37.566907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n\n#COMPILE Model label n'''\n\nmodel_rgby_label_7.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = [tf.keras.metrics.BinaryAccuracy()]\n              )\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.56829Z","iopub.status.idle":"2021-09-06T15:57:37.569051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n# Clean irrelevant variables e.g all variables != model_rgby_label_n , ds_full_ready_n, ds_full_val_ready_n\n# faire une fonction(n)\ndel ds_train_7\ndel ds_val_7\ndel ds_label_7\ndel ds_label_val_7\ndel ds_full_7\ndel ds_full_val_7\ndel model_rgb\ndel imagenet_model\n\ngc.collect()\n\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.570336Z","iopub.status.idle":"2021-09-06T15:57:37.571002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n##############################################################\n# EXECUTE TRAINING\n##############################################################\n'''\n# Remember to precise your label (n) in model name\n\nhistory=model_rgby_label_7.fit(ds_full_ready_7, epochs=15, validation_data=ds_full_val_ready_7) # only understand 1 input tensor e.g. list with 1 array \nprint(\"\\n ------ \\n\",\"training history:\",history.history)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.572567Z","iopub.status.idle":"2021-09-06T15:57:37.573312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n#model_rgby_label_2.save(\"./SAVE_MODEL/model_label_2_monolabel_7000.h5\")\n#model_rgby_label_6.save(\"./SAVE_MODEL/model_label_6_monolabel_7000.h5\")\n#model_rgby_label_7.save(\"./SAVE_MODEL/model_label_7_monolabel_7000.h5\")\n#model_rgby_label_10.save(\"./SAVE_MODEL/model_label_10_monolabel_7000.h5\")\n#model_rgby_label_13.save(\"./SAVE_MODEL/model_label_13_monolabel_7000.h5\")\n#model_rgby_label_17.save(\"./SAVE_MODEL/model_label_17_monolabel_7000.h5\")\n\nmodel_name = \"./SAVE_MODEL/model_label_{}_{}.h5\".format(n,N)\nmodel_rgby_label_7.save(model_name)\nprint(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.574727Z","iopub.status.idle":"2021-09-06T15:57:37.575397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion : \nAfter some tedious computations, we recover the 18 trained models corresponding to label 0 to 17. Here the code save directly the models into a suitable Kaggle folder. To continue this notebook, you'll find all the saved files in the folder \"Training\" on my Github\n","metadata":{}},{"cell_type":"code","source":"'''EXECUTE only if needed'''\n#model_rgby_label_7  = load_model(f'./SAVE_MODEL/model_label7.h5')\n#print(\"Done--------\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.576731Z","iopub.status.idle":"2021-09-06T15:57:37.577493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n\nfrom plotly import graph_objects as go\ncolor_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\nfig = go.Figure(data=[\n                      go.Scatter(\n                          y=history.history[\"binary_accuracy\"],\n                          name=\"Training accuracy\",\n                          mode=\"lines\",\n                          marker=dict(\n                              color=color_chart[4]\n                          )),\n                      go.Scatter(\n                          y=history.history[\"val_binary_accuracy\"],\n                          name=\"Validation accruracy\",\n                          mode=\"lines\",\n                          marker=dict(\n                              color=color_chart[5]\n                          ))\n])\nfig.update_layout(\n    title='Accuracy / Single label model (label 7) / X images ',\n    xaxis_title='epochs',\n    yaxis_title='Accuracy'    \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.578939Z","iopub.status.idle":"2021-09-06T15:57:37.579525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n\nfrom plotly import graph_objects as go\nfig = go.Figure(data=[\n                      go.Scatter(\n                          y=history.history[\"loss\"],\n                          name=\"Training loss\",\n                          mode=\"lines\",\n                          marker=dict(\n                              color=color_chart[0]\n                          )),\n                      go.Scatter(\n                          y=history.history[\"val_loss\"],\n                          name=\"Validation loss\",\n                          mode=\"lines\",\n                          marker=dict(\n                              color=color_chart[1]\n                          ))\n])\nfig.update_layout(\n    title='Loss function / Single label model (label 7) / 1000 images',\n    xaxis_title='epochs',\n    yaxis_title='Cross Entropy'    \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.580807Z","iopub.status.idle":"2021-09-06T15:57:37.581607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Predictions","metadata":{}},{"cell_type":"code","source":"data_multilabel=data_train[data_train[\"MonoLabel\"]==0]\ndata_multilabel=data_multilabel.sample(100)\n\ndata_multilabel['prediction']=data_multilabel['ID'].apply(lambda x: model_rgby.predict(np.expand_dims((open_rgby2(str(x))),0)))\n\n#predictions_test = model_rgby.predict(np.expand_dims((open_rgby2(str(x))) for x in tqdm(data_multilabel['ID']), 0))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.583098Z","iopub.status.idle":"2021-09-06T15:57:37.583786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_multilabel['prediction_lb']=data_multilabel['prediction'].apply(lambda x: 1 if x>0.5 else 0)\n                                                                                                    \ndata_multilabel['r']=data_multilabel['prediction_lb']-data_multilabel['Label_0']\ndata_multilabel","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.585011Z","iopub.status.idle":"2021-09-06T15:57:37.585665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_multilabel['r'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.587072Z","iopub.status.idle":"2021-09-06T15:57:37.587931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''EXECUTE'''\n\n# prediction test for a sample of 100 images\n# don't forget to change your label number\nn=100\ntotal=0\ntester_prediction=X_test_0.sample(n)\ntester_prediction=tester_prediction.reset_index()\nfor i in range (n):\n    ID = tester_prediction.loc[i,'ID']\n    label=tester_prediction.loc[i,'Label_0']\n    img=open_rgby(ID)\n    #img=img.astype(np.float32)/255.\n    predictions_test = model_rgby_label_0.predict(np.expand_dims(img, 0))\n    if label==1 and predictions_test>0.5:\n        total=total+1\n    #if label==0 and predictions_test<0.5:\n    #   total=total+1\n    print('prediction=', predictions_test,\"attendu:\", label)\nprint('taux succes:',(total/n)*100)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.589532Z","iopub.status.idle":"2021-09-06T15:57:37.590244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_submission_test = pd.read_csv(\"../input/hpa-single-cell-image-classification/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.591545Z","iopub.status.idle":"2021-09-06T15:57:37.592409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=[]\nN=7000\nlabels=np.arange(18)\nlabels\n#for df in range(len(sub_dataset)):\n    #print('predicting for {}'.format(df))\n    \nfor label in tqdm(labels):    \n    model.append(load_model(\"../input/single-labelmodelsaved/model_label_{}_monolabel_{}.h5\".format(label,N)))\n    submission_dataset['prediction_{}'.format(label)]=submission_dataset['ID'].apply(lambda x: model[label].predict(np.expand_dims((open_rgby(str(x))),0)))\n    submission_dataset['prediction_result_{}'.format(label)]=submission_dataset['prediction_{}'.format(label)].apply(lambda x: 1 if x>0.5 else 0) \n    \n#sub_dataset[df].to_csv('./sub_dataset_{}_{}'.format(label,df))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:57:37.593797Z","iopub.status.idle":"2021-09-06T15:57:37.594483Z"},"trusted":true},"execution_count":null,"outputs":[]}]}